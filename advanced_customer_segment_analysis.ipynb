{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oimcQVJc7zW7"
      },
      "source": [
        "# Amazon Customer Segment: Actionable Insights Report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTWi6BA07zW-"
      },
      "source": [
        "## Introduction\n",
        "This report analyzes pre-defined customer segments to answer critical business questions. The goal is to provide targeted, data-driven strategies for marketing, customer retention, and operational improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPQs2Rd57zW_",
        "outputId": "38f03d1b-b26c-44d7-f12b-cf0f84a42018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting seaborn\n",
            "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.10.6-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\user\\desktop\\ml mini proj - copy\\.venv\\lib\\site-packages (from seaborn) (2.3.3)\n",
            "Requirement already satisfied: pandas>=1.2 in c:\\users\\user\\desktop\\ml mini proj - copy\\.venv\\lib\\site-packages (from seaborn) (2.3.2)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Using cached contourpy-1.3.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Using cached fonttools-4.60.1-cp313-cp313-win_amd64.whl.metadata (114 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Using cached kiwisolver-1.4.9-cp313-cp313-win_amd64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\desktop\\ml mini proj - copy\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Collecting pillow>=8 (from matplotlib)\n",
            "  Using cached pillow-11.3.0-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\desktop\\ml mini proj - copy\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\desktop\\ml mini proj - copy\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\desktop\\ml mini proj - copy\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\desktop\\ml mini proj - copy\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "Using cached matplotlib-3.10.6-cp313-cp313-win_amd64.whl (8.1 MB)\n",
            "Using cached contourpy-1.3.3-cp313-cp313-win_amd64.whl (226 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.60.1-cp313-cp313-win_amd64.whl (2.3 MB)\n",
            "Using cached kiwisolver-1.4.9-cp313-cp313-win_amd64.whl (73 kB)\n",
            "Using cached pillow-11.3.0-cp313-cp313-win_amd64.whl (7.0 MB)\n",
            "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
            "\n",
            "   ---------------------------------------- 0/8 [pyparsing]\n",
            "   ---------------------------------------- 0/8 [pyparsing]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   ----- ---------------------------------- 1/8 [pillow]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   --------------- ------------------------ 3/8 [fonttools]\n",
            "   -------------------- ------------------- 4/8 [cycler]\n",
            "   ------------------------- -------------- 5/8 [contourpy]\n",
            "   ------------------------- -------------- 5/8 [contourpy]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ------------------------------ --------- 6/8 [matplotlib]\n",
            "   ----------------------------------- ---- 7/8 [seaborn]\n",
            "   ----------------------------------- ---- 7/8 [seaborn]\n",
            "   ----------------------------------- ---- 7/8 [seaborn]\n",
            "   ----------------------------------- ---- 7/8 [seaborn]\n",
            "   ----------------------------------- ---- 7/8 [seaborn]\n",
            "   ----------------------------------- ---- 7/8 [seaborn]\n",
            "   ----------------------------------- ---- 7/8 [seaborn]\n",
            "   ----------------------------------- ---- 7/8 [seaborn]\n",
            "   ---------------------------------------- 8/8 [seaborn]\n",
            "\n",
            "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.6 pillow-11.3.0 pyparsing-3.2.5 seaborn-0.13.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# --- Setup and Imports ---\n",
        "%pip install seaborn matplotlib\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Set plot style\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "# --- Output Directory Setup ---\n",
        "output_dir = 'output_2'\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sSkNYHj7zXB",
        "outputId": "2b6bf6e7-687a-4bb9-9199-cd2e5171f016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File not found: [Errno 2] No such file or directory: 'outputs/amazon_customer_clusters.csv'\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'outputs/amazon_customer_clusters.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m original_data_file = \u001b[33m'\u001b[39m\u001b[33moutputs/Amazon Customer Behavior Survey.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclustered_data_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     df_original = pd.read_csv(original_data_file)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\ML MINI PROJ - Copy\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\ML MINI PROJ - Copy\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\ML MINI PROJ - Copy\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\ML MINI PROJ - Copy\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\ML MINI PROJ - Copy\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'outputs/amazon_customer_clusters.csv'"
          ]
        }
      ],
      "source": [
        "# --- Load Data ---\n",
        "# Try multiple options and auto-detect if needed\n",
        "cluster_candidates = [\n",
        "    'outputs/amazon_customer_clusters.csv',\n",
        "    'outputs/amazon_customers_annotated.csv',\n",
        "]\n",
        "\n",
        "original_candidates = [\n",
        "    'outputs/Amazon Customer Behavior Survey.csv',\n",
        "]\n",
        "\n",
        "# Also scan any csvs in outputs/ as fallback\n",
        "outputs_csvs = glob.glob('outputs/*.csv')\n",
        "\n",
        "def pick_cluster_file(candidates, fallbacks):\n",
        "    for path in candidates:\n",
        "        if os.path.exists(path):\n",
        "            return path\n",
        "    # Fallback: look for required columns\n",
        "    for path in fallbacks:\n",
        "        try:\n",
        "            sample = pd.read_csv(path, nrows=50)\n",
        "            if 'Cluster' in sample.columns:\n",
        "                return path\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise FileNotFoundError(\"Could not find a clustered data CSV in 'outputs/'.\")\n",
        "\n",
        "def pick_original_file(candidates, fallbacks):\n",
        "    for path in candidates:\n",
        "        if os.path.exists(path):\n",
        "            return path\n",
        "    # Fallback: any CSV is acceptable as long as it has Product_Search_Method\n",
        "    for path in fallbacks:\n",
        "        try:\n",
        "            sample = pd.read_csv(path, nrows=50)\n",
        "            if 'Product_Search_Method' in sample.columns:\n",
        "                return path\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise FileNotFoundError(\"Could not find the original survey CSV in 'outputs/'.\")\n",
        "\n",
        "clustered_data_file = pick_cluster_file(cluster_candidates, outputs_csvs)\n",
        "original_data_file = pick_original_file(original_candidates, outputs_csvs)\n",
        "\n",
        "print(f\"Using clustered data: {clustered_data_file}\")\n",
        "print(f\"Using original data:  {original_data_file}\")\n",
        "\n",
        "# Load\n",
        "try:\n",
        "    df = pd.read_csv(clustered_data_file)\n",
        "    df_original = pd.read_csv(original_data_file)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"File not found: {e}\")\n",
        "    raise\n",
        "\n",
        "# Display preview of main dataframe\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R55bG7D67zXC"
      },
      "source": [
        "## Analysis 1: Prime Membership Candidates Identifier\n",
        "**Business Question:** Which non-Prime members are the most valuable targets for a Prime membership marketing campaign?\n",
        "\n",
        "**Methodology:** We identify customers who are not currently Prime members but exhibit behaviors of a loyal customer (i.e., high purchase frequency).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35daIBLg7zXD"
      },
      "outputs": [],
      "source": [
        "# --- Analysis 1: Prime Candidates Function ---\n",
        "from typing import Tuple\n",
        "\n",
        "def identify_prime_candidates(data: pd.DataFrame, frequency_threshold: int = 3) -> pd.DataFrame:\n",
        "    \"\"\"Return customers who are not Prime members but have high purchase frequency.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : pd.DataFrame\n",
        "        Input dataframe containing Membership_Status and Purchase_Frequency columns.\n",
        "    frequency_threshold : int, default 3\n",
        "        Minimum purchase frequency to qualify as a candidate.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Filtered dataframe of candidate customers.\n",
        "    \"\"\"\n",
        "    required_cols = {\"Membership_Status\", \"Purchase_Frequency\"}\n",
        "    missing = required_cols - set(data.columns)\n",
        "    if missing:\n",
        "        raise KeyError(f\"Missing required columns for Analysis 1: {missing}\")\n",
        "\n",
        "    candidates = data[(data[\"Membership_Status\"] == \"Not a Member\") & (data[\"Purchase_Frequency\"] >= frequency_threshold)]\n",
        "    return candidates.copy()\n",
        "\n",
        "# Execute Analysis 1\n",
        "prime_candidates = identify_prime_candidates(df, frequency_threshold=3)\n",
        "print(f\"Prime candidates found: {len(prime_candidates)}\")\n",
        "prime_candidates_path = os.path.join(output_dir, 'prime_candidates.csv')\n",
        "prime_candidates.to_csv(prime_candidates_path, index=False)\n",
        "prime_candidates.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTPe2sEq7zXD"
      },
      "source": [
        "## Analysis 2: \"At-Risk\" Churn Scorecard\n",
        "**Business Question:** Which customer segment is most likely to stop using our service (churn)?\n",
        "\n",
        "**Methodology:** We create a scorecard for each cluster based on a combination of low shopping satisfaction, high return frequency, and a high number of customer service interactions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je_FelTJ7zXE"
      },
      "outputs": [],
      "source": [
        "# --- Analysis 2: At-Risk Scorecard Function ---\n",
        "def calculate_at_risk_scorecard(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Compute cluster-level risk indicators.\n",
        "\n",
        "    Returns a dataframe with average Shopping_Satisfaction, Return_Frequency,\n",
        "    and Customer_Service_Interactions by Cluster.\n",
        "    \"\"\"\n",
        "    required_cols = {\"Cluster\", \"Shopping_Satisfaction\", \"Return_Frequency\", \"Customer_Service_Interactions\"}\n",
        "    missing = required_cols - set(data.columns)\n",
        "    if missing:\n",
        "        raise KeyError(f\"Missing required columns for Analysis 2: {missing}\")\n",
        "\n",
        "    scorecard = (\n",
        "        data.groupby(\"Cluster\", as_index=False)[\n",
        "            [\"Shopping_Satisfaction\", \"Return_Frequency\", \"Customer_Service_Interactions\"]\n",
        "        ]\n",
        "        .mean()\n",
        "        .rename(columns={\n",
        "            \"Shopping_Satisfaction\": \"Avg_Shopping_Satisfaction\",\n",
        "            \"Return_Frequency\": \"Avg_Return_Frequency\",\n",
        "            \"Customer_Service_Interactions\": \"Avg_Customer_Service_Interactions\"\n",
        "        })\n",
        "    )\n",
        "    return scorecard\n",
        "\n",
        "# Execute Analysis 2\n",
        "at_risk_scorecard = calculate_at_risk_scorecard(df)\n",
        "print(at_risk_scorecard)\n",
        "at_risk_path = os.path.join(output_dir, 'at_risk_scorecard.csv')\n",
        "at_risk_scorecard.to_csv(at_risk_path, index=False)\n",
        "at_risk_scorecard.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzNFPcpH7zXE"
      },
      "source": [
        "## Analysis 3: Targeted Advertising Channel Advisor\n",
        "**Business Question:** Where should we spend our advertising budget to most effectively reach each customer segment?\n",
        "\n",
        "**Methodology:** We analyze the most commonly used product search method for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWRgXBDX7zXF"
      },
      "outputs": [],
      "source": [
        "# --- Analysis 3: Ad Advisor Function and Visualization ---\n",
        "from statistics import mode\n",
        "\n",
        "def get_top_search_methods(data: pd.DataFrame, original_data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Join Product_Search_Method from original data and compute cluster modes.\n",
        "\n",
        "    Returns a dataframe with the most common Product_Search_Method per Cluster.\n",
        "    \"\"\"\n",
        "    # Determine join key(s). Assume both have a stable unique index if present; else fall back to row order.\n",
        "    left = data.reset_index(drop=True)\n",
        "    right = original_data.reset_index(drop=True)\n",
        "\n",
        "    if \"Product_Search_Method\" not in right.columns:\n",
        "        raise KeyError(\"'Product_Search_Method' not found in original_data\")\n",
        "\n",
        "    merged = left.copy()\n",
        "    merged[\"Product_Search_Method\"] = right[\"Product_Search_Method\"]\n",
        "\n",
        "    # Compute mode per cluster; handle multimodal by choosing the first.\n",
        "    top_methods = (\n",
        "        merged.groupby(\"Cluster\")[\"Product_Search_Method\"]\n",
        "        .agg(lambda s: s.mode().iat[0] if not s.mode().empty else None)\n",
        "        .reset_index()\n",
        "        .rename(columns={\"Product_Search_Method\": \"Top_Product_Search_Method\"})\n",
        "    )\n",
        "\n",
        "    return top_methods, merged\n",
        "\n",
        "# Execute Analysis 3\n",
        "top_methods, df_with_search = get_top_search_methods(df, df_original)\n",
        "print(top_methods)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df_with_search, x=\"Product_Search_Method\", hue=\"Cluster\")\n",
        "plt.title(\"Distribution of Product Search Methods by Cluster\")\n",
        "plt.xlabel(\"Product Search Method\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=30, ha='right')\n",
        "plt.tight_layout()\n",
        "advertising_plot_path = os.path.join(output_dir, 'advertising_channels.png')\n",
        "plt.savefig(advertising_plot_path, dpi=300)\n",
        "plt.close()\n",
        "\n",
        "top_methods.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKUs2mB87zXG"
      },
      "source": [
        "## Analysis 4: Recommendation Engine Effectiveness Check\n",
        "**Business Question:** Does using our personalized recommendations correlate with higher customer satisfaction?\n",
        "\n",
        "**Methodology:** We compare the average shopping satisfaction for customers who use recommendations 'Often' versus those who use them 'Rarely'/'Never'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTO5HsM-7zXG"
      },
      "outputs": [],
      "source": [
        "# --- Analysis 4: Recommendation Effectiveness Function and Visualization ---\n",
        "\n",
        "def check_recommendation_effectiveness(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Compare average Shopping_Satisfaction between high vs low recommendation usage groups.\n",
        "\n",
        "    Categorize 'Personalized_Recommendation_Frequency' into High Usage vs Low Usage and\n",
        "    compute mean Shopping_Satisfaction by group.\n",
        "    \"\"\"\n",
        "    required_cols = {\"Personalized_Recommendation_Frequency\", \"Shopping_Satisfaction\"}\n",
        "    missing = required_cols - set(data.columns)\n",
        "    if missing:\n",
        "        raise KeyError(f\"Missing required columns for Analysis 4: {missing}\")\n",
        "\n",
        "    usage_map = {\n",
        "        \"Often\": \"High Usage\",\n",
        "        \"Frequently\": \"High Usage\",\n",
        "        \"Always\": \"High Usage\",\n",
        "        \"Sometimes\": \"Low Usage\",\n",
        "        \"Rarely\": \"Low Usage\",\n",
        "        \"Never\": \"Low Usage\"\n",
        "    }\n",
        "\n",
        "    temp = data.copy()\n",
        "    temp[\"Recommendation_Usage\"] = temp[\"Personalized_Recommendation_Frequency\"].map(usage_map).fillna(\"Low Usage\")\n",
        "\n",
        "    comparison = (\n",
        "        temp.groupby(\"Recommendation_Usage\", as_index=False)[\"Shopping_Satisfaction\"].mean()\n",
        "        .rename(columns={\"Shopping_Satisfaction\": \"Avg_Shopping_Satisfaction\"})\n",
        "    )\n",
        "\n",
        "    return comparison, temp\n",
        "\n",
        "# Execute Analysis 4\n",
        "rec_effectiveness, df_with_usage = check_recommendation_effectiveness(df)\n",
        "print(rec_effectiveness)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=rec_effectiveness, x=\"Recommendation_Usage\", y=\"Avg_Shopping_Satisfaction\", palette=\"viridis\")\n",
        "plt.title(\"Shopping Satisfaction by Recommendation Usage\")\n",
        "plt.xlabel(\"Recommendation Usage\")\n",
        "plt.ylabel(\"Average Shopping Satisfaction\")\n",
        "plt.tight_layout()\n",
        "rec_plot_path = os.path.join(output_dir, 'recommendation_effectiveness.png')\n",
        "plt.savefig(rec_plot_path, dpi=300)\n",
        "plt.close()\n",
        "\n",
        "rec_effectiveness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1fA0C4P7zXH"
      },
      "source": [
        "## Analysis 5: Customer Service Demand Forecaster\n",
        "**Business Question:** Which customer segment requires the most attention from our customer service team?\n",
        "\n",
        "**Methodology:** We calculate the average number of customer service interactions for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LusrydDM7zXJ"
      },
      "outputs": [],
      "source": [
        "# --- Analysis 5: Service Demand Function and Visualization ---\n",
        "\n",
        "def forecast_service_demand(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Compute average customer service interactions per cluster.\"\"\"\n",
        "    required_cols = {\"Cluster\", \"Customer_Service_Interactions\"}\n",
        "    missing = required_cols - set(data.columns)\n",
        "    if missing:\n",
        "        raise KeyError(f\"Missing required columns for Analysis 5: {missing}\")\n",
        "\n",
        "    demand = (\n",
        "        data.groupby(\"Cluster\", as_index=False)[\"Customer_Service_Interactions\"].mean()\n",
        "        .rename(columns={\"Customer_Service_Interactions\": \"Avg_Customer_Service_Interactions\"})\n",
        "    )\n",
        "    return demand\n",
        "\n",
        "# Execute Analysis 5\n",
        "service_demand = forecast_service_demand(df)\n",
        "print(service_demand)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=service_demand, x=\"Cluster\", y=\"Avg_Customer_Service_Interactions\", palette=\"mako\")\n",
        "plt.title(\"Forecast: Customer Service Interactions by Cluster\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Average Customer Service Interactions\")\n",
        "plt.tight_layout()\n",
        "service_plot_path = os.path.join(output_dir, 'service_demand_forecast.png')\n",
        "plt.savefig(service_plot_path, dpi=300)\n",
        "plt.close()\n",
        "\n",
        "service_demand.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoL8eH0K7zXK"
      },
      "source": [
        "## Report Conclusion\n",
        "The analyses above provide several actionable insights for enhancing business performance.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}